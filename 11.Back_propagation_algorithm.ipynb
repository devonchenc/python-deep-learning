{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一章介绍了神经网络的学习，并通过数值微分计算了神经网络中损失函数关于权重参数的梯度）。数值微分虽然简单，也容易实现，但缺点是计算上比较费时间。本章我们将学习一个能够高效计算权重参数的梯度的方法——误差反向传播法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要正确理解误差反向传播法，有两种方法：\n",
    "\n",
    "- 基于数学式\n",
    "- 基于计算图（computational graph）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 链式法则\n",
    "\n",
    "微积分中的链式法则用于计算复合函数的导数，反向传播是一种计算链式法则的运算，使用高效的特定运算顺序。\n",
    "\n",
    "设 $x$ 是实数，$f$ 和 $g$ 是实数映射到实数的函数。假设 $y=g(x)$ 并且 $z = f(g(x))=f(y)$，那么链式法则为\n",
    "\n",
    "$$ \\frac{dz}{dx} = \\frac{dz}{dy}\\frac{dy}{dx} $$\n",
    "\n",
    "我们可以将其从标量扩展到向量。假设 $X\\in \\mathbb{R}^m$，$Y\\in \\mathbb{R}^n$，$g$ 是从 $\\mathbb{R}^m$ 到 $\\mathbb{R}^n$ 的映射，$f$ 是从 $\\mathbb{R}^n$ 到 $\\mathbb{R}$ 的映射。如果$Y = g(X)$ 并且 $Z = f(Y)$，那么\n",
    "\n",
    "$$ \\frac{\\partial Z}{\\partial X} = \\sum_j \\frac{\\partial Z}{\\partial Y_j}\\frac{\\partial Y_j}{\\partial X_i} $$\n",
    "\n",
    "使用向量记法，可以等价地写成\n",
    "\n",
    "$$ \\nabla_X Z=(\\frac{\\partial Y}{\\partial X})^\\top \\nabla_Y Z$$\n",
    "\n",
    "其中，$\\frac{\\partial Y}{\\partial X}$ 是 $g$ 的 $n\\times m$ 的 Jacobian 矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前向传播\n",
    "\n",
    "首先给出典型神经网络中的前向传播和**代价函数**的计算过程。代价函数等于损失函数加上正则项 $\\Omega(\\theta)$，其中 $\\theta$ 包含所有参数（权重和偏置），损失函数 $L(\\hat y, y)$ 取决于神经网络输出 $\\hat y$ 和目标 $y$。\n",
    "\n",
    "$$ J = L(\\hat y, y) + \\lambda \\Omega(\\theta) $$\n",
    "\n",
    "Require: 网络深度 $l$\n",
    "\n",
    "Require: $W^{(i)}, i\\in {1,...,l}$，模型的权重矩阵\n",
    "\n",
    "Require: $b^{(i)}, i\\in {1,...,l}$，模型的偏置参数\n",
    "\n",
    "Require: $x$，程序的输入\n",
    "\n",
    "Require: $y$，目标输出\n",
    "\n",
    "$h^{(0)} = x$\n",
    "\n",
    "$ {\\rm for \\;} k=1,...,l {\\rm \\; do}$\n",
    "\n",
    "$ \\qquad a^{(k)} = b^{(k)} + W^{(k)}h^{(k-1)}$\n",
    "\n",
    "$ \\qquad h^{(k)} = f(a^{(k)})$\n",
    "\n",
    "$ {\\rm end \\; for} $\n",
    "\n",
    "$\\hat y = h(l)$\n",
    "\n",
    "$ J = L(\\hat y, y) + \\lambda \\Omega(\\theta) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播\n",
    "\n",
    "该计算对于每一层 $k$ 都产生了对激活 $a^{(k)} $ 的梯度，从输出层开始向后计算一直到第一个隐藏层。这些梯度可以看作对每层的输出应如何调整以减小误差的指导，根据这些梯度可以获得每层权重和偏置的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ g \\leftarrow \\nabla_{\\hat y}J = \\nabla_{\\hat y}L(\\hat y, y) $\n",
    "\n",
    "$ {\\rm for \\;} k=l,l-1,...,l {\\rm \\; do}$\n",
    "\n",
    "     将关于层输出的梯度转换为非线性激活输入前的梯度：\n",
    "\n",
    "$ \\qquad g \\leftarrow \\nabla_{a^{(k)}}J = g f'(a^{(k)})$\n",
    "\n",
    "\n",
    "     计算关于权重和偏置的梯度：\n",
    "$ \\qquad \\nabla_{W^{(k)}} J = g h^{(k-1)\\top} + \\lambda \\nabla_{W^{(k)}}\\Omega(\\theta)$\n",
    "\n",
    "$ \\qquad \\nabla_{b^{(k)}} J = g + \\lambda \\nabla_{b^{(k)}}\\Omega(\\theta)$\n",
    "\n",
    "     关于下一更低层的隐藏层传播梯度：\n",
    "$ \\qquad g \\leftarrow \\nabla_{h^{(k-1)}}J = W^{(k)\\top}g$\n",
    "\n",
    "$ {\\rm end \\; for} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 计算图\n",
    "## 2.1 用计算图求解\n",
    "\n",
    "**计算图**（computational graph） 是表达和评估数学表达式的一种方式，它被定义为有向图，其中节点对应于数学运算。\n",
    "\n",
    "对于方程 $ p = x + y $ 的计算图如下：\n",
    "\n",
    "![img](images/chapter11/computational_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题1： 在超市买了 2 个 100 元一个的苹果，额外的消费税是 10%，请计算支付金额。\n",
    "\n",
    "![img](images/chapter11/apple_forward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "综上，用计算图解题的情况下，需要按如下流程进行。\n",
    "\n",
    "\n",
    "1. 构建计算图。\n",
    "\n",
    "\n",
    "2. 在计算图上，从左向右进行计算。\n",
    "\n",
    "这里的第2 歩“从左向右进行计算”是正向传播（forward propagation）。正向传播是从计算图出发点到结束点的传播。而从右向左的传播被称为反向传播（backward propagation），它将在接下来的导数计算中发挥重要作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 为何用计算图解题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 无论全局是多么复杂的计算，都可以通过局部计算使各个节点致力于简单的计算，从而简化问题。\n",
    "\n",
    "\n",
    "- 利用计算图可以将中间的计算结果全部保存起来。\n",
    "\n",
    "\n",
    "- 可以通过反向传播高效计算导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，假设我们想知道苹果价格的上涨会在多大程度上影响最终的支付金额，即求“支付金额关于苹果的价格的导数”。设苹果的价格为 $x$，支付金额为 $L$，则相当于求 $\\frac{\\partial L}{\\partial x}$。导数的数值可以通过计算图的反向传播求出来。\n",
    "\n",
    "\n",
    "![img](images/chapter11/apple_backward.png)\n",
    "\n",
    "从这个结果中可知，“支付金额关于苹果的价格的导数”的值是2.2。这意味着，如果苹果的价格上涨 1 元，最终的支付金额会增加 2.2 元。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里只求了关于苹果的价格的导数，不过“支付金额关于消费税的导数”、“支付金额关于苹果的个数的导数”等也都可以用同样的方式算出来。并且，计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数。\n",
    "\n",
    "综上，计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. 计算图中的链式法则\n",
    "\n",
    "复合函数是由多个函数构成的函数。比如，$z = (x + y)^2$ 是由下述两个公式构成：\n",
    "\n",
    "$$ z= t^2 $$\n",
    "$$ t = x + y $$\n",
    "\n",
    "如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示。\n",
    "\n",
    "$z$ 关于 $x$ 的导数$ \\frac{\\partial z}{\\partial x}$，可以用 $z$ 关于 $t$ 的导数 $\\frac{\\partial z}{\\partial t}$ 和 $t$ 关于 $x$ 的导数 $\\frac{\\partial t}{\\partial x}$ 的乘积表示。\n",
    "\n",
    "$$ \\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t}\\frac{\\partial t}{\\partial x} = 2t\\cdot 1 = 2(x+y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算图的反向传播从右到左传播信号。反向传播的计算顺序是，先将节点的输入信号乘以节点的局部导数（偏导数），然后再传递给下一个节点。\n",
    "\n",
    "![img](images/chapter11/chain1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将求导结果带入上图中得到，\n",
    "![img](images/chapter11/chain2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 加法层的反向传播\n",
    "\n",
    "以 $z = x + y$ 为对象，观察它的反向传播。$z = x + y$ 的导数可解析性地计算出来。\n",
    "\n",
    "$$ \\frac{\\partial z}{\\partial x} = 1$$\n",
    "\n",
    "$$ \\frac{\\partial z}{\\partial y} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播将从上游传过来的导数（本例中是$ \\frac{\\partial L}{\\partial z}$）乘以1，然后传向下游。也就是说，因为加法节点的反向传播只乘以1，所以输入的值会原封不动地流向下一个节点。\n",
    "\n",
    "![img](images/chapter11/add_node.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们实现加法节点的加法层，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # 正向传播\n",
    "    def forward(self, x, y):\n",
    "        return x + y\n",
    "\n",
    "    # 反向传播\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        # return dout, dout\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加法层不需要特意进行初始化，所以 \\_\\_init\\_\\_() 中什么也不运行（pass语句表示“什么也不运行”）。加法层的 forward() 接收 x 和 y 两个参数，将它们相加后输出。backward() 将上游传来的导数（dout）原封不动地传递给下游。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 乘法层的反向传播\n",
    "\n",
    "这里我们考虑 $z = xy$。这个式的导数用下式表示：\n",
    "\n",
    "$$ \\frac{\\partial z}{\\partial x} = y$$\n",
    "\n",
    "$$ \\frac{\\partial z}{\\partial y} = x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "乘法的反向传播会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游。翻转值表示一种翻转关系，如下图所示，正向传播时信号是 x 的话，反向传播时则是 y；正向传播时信号是 y 的话，反向传播时则是 x。\n",
    "\n",
    "![img](images/chapter11/multi_node.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    \n",
    "    # 正向传播\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return x * y\n",
    "\n",
    "    # 反向传播\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y # 翻转x和y\n",
    "        dy = dout * self.x\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\_\\_init\\_\\_()中会初始化实例变量 x 和 y ，它们用于保存正向传播时的输入值。forward() 接收 x 和 y 两个参数，将它们相乘后输出。backward()将从上游传来的导数（dout）乘以正向传播的翻转值，然后传给下游。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们使用 MulLayer 类实现前面的购买苹果的例子：\n",
    "\n",
    "![img](images/chapter11/apple_backward_result.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(price) # 220"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于各个变量的导数可由backward()求出。这里调用backward()的顺序与调用forward()的顺序相反。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 200\n",
      "2.2 110.00000000000001\n"
     ]
    }
   ],
   "source": [
    "# backward\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "print(dapple_price, dtax)\n",
    "\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "print(dapple, dapple_num) # 2.2 110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 激活函数层的反向传播\n",
    "\n",
    "现在，我们将计算图的思路应用到神经网络中。这里，我们把构成神经网络的层实现为一个类。先来实现激活函数的 ReLU 层和 Sigmoid 层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 ReLU层\n",
    "\n",
    "激活函数ReLU（Rectified Linear Unit）由下式表示，\n",
    "\n",
    "$$ y = \\begin{cases}\n",
    "x&x > 0 \\\\\n",
    "0&x <= 0\n",
    "\\end{cases} $$\n",
    "\n",
    "可以求出 $y$ 关于 $x$ 的导数，\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial x} = \\begin{cases}\n",
    "1&x > 0 \\\\\n",
    "0&x <= 0\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果正向传播时的输入 $x$ 大于0，则反向传播会将上游的值原封不动地传给下游。反过来，如果正向传播时的 $x$ 小于等于0，则反向传播中传给下游的信号将停在此处。计算图如图所示，\n",
    "\n",
    "![img](images/chapter11/relu.png)\n",
    "\n",
    "现在我们来实现ReLU层。在神经网络的层的实现中，一般假定 forward() 和 backward() 的参数是NumPy数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu 类有实例变量 mask，这个变量是由 True/False 构成的 NumPy 数组，它会把正向传播时的输入 $x$ 的元素中小于等于 0 的地方保存为 True，其它地方（大于 0 的元素）保存为 False。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 3.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array( [[1.0, -0.5], [-2.0, 3.0]] )\n",
    "\n",
    "Relu().forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Sigmoid层\n",
    "\n",
    "sigmoid 函数公式为\n",
    "$$ y = \\frac{1}{1 + \\exp (-x)} $$\n",
    "\n",
    "用计算图表示的话如图所示，\n",
    "\n",
    "![img](images/chapter11/sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们进行计算图的反向传播。\n",
    "\n",
    "- 步骤1\n",
    "      “/”节点表示 y = 1/x，它的导数可以解析性地表示为下式。\n",
    "$$\\frac{\\partial y}{\\partial x} = - \\frac{1}{x^2} = -y^2$$\n",
    "- 步骤2\n",
    "      “+”节点将上游的值原封不动地传给下游。\n",
    "- 步骤3\n",
    "      “exp”节点表示 y = exp(x)，它的导数为\n",
    "$$ \\frac{\\partial y}{\\partial x} = \\exp(x) $$\n",
    "- 步骤4\n",
    "      “×”节点将正向传播时的值翻转后做乘法运算。因此，这里要乘以−1。\n",
    "\n",
    "因此，Sigmoid层的反向传播计算图为：\n",
    "\n",
    "\n",
    "![img](images/chapter11/sigmoid_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid 层的反向传播输出为 $\\frac{\\partial L}{\\partial y}y^2\\exp(-x)$，可以进一步整理如下：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial y}y^2\\exp(-x) = \\frac{\\partial L}{\\partial y}y\\frac{1}{1 + \\exp (-x)}\\exp(-x) = \\frac{\\partial L}{\\partial y}y(1-y)$$\n",
    "\n",
    "![img](images/chapter11/sigmoid_result_simple.png)\n",
    "\n",
    "用 Python 代码实现sigmoid层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None    # 保存正向传播结果\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.out = 1 / (1 + np.exp(-x))\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.out * (1.0 - self.out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 全连接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑一个简单的神经网络全连接层计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(2) # 输入\n",
    "W = np.random.rand(2,3) # 权重\n",
    "B = np.random.rand(3) # 偏置\n",
    "\n",
    "Y = np.dot(X, W) + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个计算过程可以用计算图表示出来。之前我们见到的计算图中各个节点间流动的是标量，而这个例子中各个节点间传播的是矩阵。\n",
    "\n",
    "![img](images/chapter11/affine_forward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以矩阵为对象的反向传播，按矩阵的各个元素进行计算时，步骤和以标量为对象的计算图相同。\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y}\\cdot W^T $$\n",
    "$$ \\frac{\\partial L}{\\partial W} = X^T\\cdot \\frac{\\partial L}{\\partial Y} $$\n",
    "\n",
    "神经网络计算图的反向传播：\n",
    "\n",
    "![img](images/chapter11/affine_backward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们考虑 N 个数据一起进行正向传播的情况，也就是批版本的全连接层。先给出批版本的全连接层的计算图，\n",
    "\n",
    "![img](images/chapter11/affine_batch_backward.png)\n",
    "\n",
    "与之前不同的是，现在输入 $X$ 的形状是(N, 2)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Softmax-with-loss层\n",
    "\n",
    "下面来实现 Softmax 层。考虑到这里也包含作为损失函数的交叉熵误差（cross entropy error），所以称为“Softmax-with-Loss 层”。Softmax-with-Loss 层（Softmax函数和交叉熵误差）的计算图如图所示。\n",
    "\n",
    "![img](images/chapter11/softmax_with_loss.png)\n",
    "\n",
    "Softmax 层将输入值归一化（将输出值的和调整为1）之后再输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 正向传播\n",
    "\n",
    "softmax 函数可由下式表示\n",
    "\n",
    "$$ y_k = \\frac{\\exp (a_k)}{\\sum_{i=1}^n \\exp (a_i)} $$\n",
    "\n",
    "在Softmax层的计算图中，指数的和记做 $S$，最终的输出记为 $(y_1,y_2,y_3)$\n",
    "\n",
    "交叉熵误差(cross entropy error)可由下式表示，\n",
    "\n",
    "$$ L = -\\sum_k t_k \\log y_k$$\n",
    "\n",
    "交叉熵误差层的最终输出记为 $ -(t_1 \\log y_1 + t_2 \\log y_2 + t_3 \\log y_3) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 反向传播\n",
    "\n",
    "首先是 Cross Entropy Error 层的反向传播，\n",
    "![img](images/chapter11/cross_entropy_backward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中注意“ log”节点的反向传播遵从：\n",
    "$$ y= log(x)$$\n",
    "$$ \\frac{\\partial y}{\\partial x} = \\frac{1}{x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是Softmax 层的反向传播的步骤，稍有些复杂。我们将它分为两部分来讨论，\n",
    "\n",
    "![img](images/chapter11/softmax_backward1.png)\n",
    "\n",
    "1. “×”节点将正向传播的值翻转后相乘。\n",
    "\n",
    "$$ -\\frac{t_1}{y1}\\exp(a_1) = -t_1\\frac{S}{\\exp (a_1)}\\exp(a_1)=-t_1 S $$\n",
    "\n",
    "2. 正向传播时若有分支流出，则反向传播时它们的反向传播的值会相加。\n",
    "\n",
    "$$ (-t_1S - t_2S - t_3S)\\cdot -(\\frac{1}{S})^2 = \\frac{1}{S}(t_1+t_2+t_3) = \\frac{1}{S} $$\n",
    "\n",
    "这里，(t1, t2, t3) 是 one-hot 向量，意味着(t1, t2, t3)中只有一个元素是1，其余都是0。因此，$t_1+t_2+t_3 = 1$\n",
    "\n",
    "3. “+”节点原封不动地传递上游的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](images/chapter11/softmax_backward2.png)\n",
    "\n",
    "1. “×”节点将值翻转后相乘。\n",
    "\n",
    "$$ -\\frac{t_1}{y_1} \\frac{1}{S} = -\\frac{t_1}{\\exp a_1} $$\n",
    "\n",
    "2. 根据“exp”节点的关系式 $ \\frac{\\partial y}{\\partial x} = \\exp(x) $，向两个分支输入的和乘以 $\\exp(a_1)$ 后的值就是我们要求的反向传播。\n",
    "\n",
    "$$ (\\frac{1}{S} - \\frac{t_1}{\\exp(a_1)}) \\exp(a_1) = \\frac{\\exp(a_1)}{S} - t_1 = y_1 - t_1 $$\n",
    "\n",
    "3. 剩下的 $a_2$、$a_3$ 也可以按照相同的步骤求出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax 层的反向传播得到了 $(y_1 − t_1, y_2 − t_2, y_3 − t_3)$ 这样“漂亮”的结果。由于 $(y1, y2, y3)$ 是 Softmax 层的输出，$(t1, t2, t3)$ 是标签数据，所以 $(y_1 − t_1, y_2 − t_2, y_3 − t_3)$ 是 Softmax 层的输出和标签的差分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如，Softmax 层的输出是 $y=(0.3, 0.2, 0.5)$ ，标签数据是 $t =(0, 1, 0)$。因为正确解标签处的概率是 0.2（20%），这时的神经网络未能进行正确的识别。此时，Softmax 层的反向传播传递的是 (0.3, −0.8, 0.5) 这样一个大的误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在来进行Softmax-with-Loss 层的实现，实现代码如下所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 损失\n",
    "        self.y = None # softmax的输出\n",
    "        self.t = None # 标签数据（one-hot vector）\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意反向传播时，将要传播的值除以批的大小（batch_size）后，传递给前面的层的是单个数据的误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 误差反向传播的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回顾神经网络学习的步骤如下：\n",
    "- 前提\n",
    "      神经网络中有合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为学习。神经网络的学习分为下面 4 个步骤。\n",
    "      \n",
    "- 步骤1（mini-batch）\n",
    "      从训练数据中随机选择一部分数据。\n",
    "\n",
    "- 步骤2（计算梯度）\n",
    "      计算损失函数关于各个权重参数的梯度。\n",
    "\n",
    "- 步骤3（更新参数）\n",
    "      将权重参数沿梯度方向进行微小的更新。\n",
    "\n",
    "- 步骤4（重复）\n",
    "      重复步骤1、步骤2、步骤3。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一章中，我们利用数值微分求得了权重参数的梯度。数值微分虽然实现简单，但是计算要耗费较多的时间。和需要花费较多时间的数值微分不同，误差反向传播法可以快速高效地计算梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 神经网络的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过像组装乐高积木一样组装上一节中实现的层，可以构建神经网络。本节我们将通过组装已经实现的层来构建神经网络，这里我们将 2 层神经网络实现为TwoLayerNet 类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TwoLayerNet类的方法：\n",
    "\n",
    "|方法|说明|\n",
    "|----|----|\n",
    "|__init__(self, input_size, hidden_size, output_size)|进行初始化。参数从头开始依次表示输入层的神经元数、隐藏层的神经元数、输出层的神经元数|\n",
    "|predict(self, x)|进行识别（推理）。参数 x 是图像数据|\n",
    "|loss(self, x, t)|计算损失函数的值。参数 x 是图像数据，t 是正确解标签|\n",
    "|accuracy(self, x, t) |计算识别精度|\n",
    "|gradient(self, x, t) | 通过误差反向传播法计算关于权重参数的梯度|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 生成层\n",
    "        self.layers = OrderedDict()   # 有序字典\n",
    "        self.layers['FC1'] = FullyConnected(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['FC2'] = FullyConnected(self.params['W2'], self.params['b2'])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    # x:输入数据, t:监督数据\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 :\n",
    "            t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    # x:输入数据, t:标签数据\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 设定\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['FC1'].dW\n",
    "        grads['b1'] = self.layers['FC1'].db\n",
    "        grads['W2'] = self.layers['FC2'].dW\n",
    "        grads['b2'] = self.layers['FC2'].db\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 溢出对策\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OrderedDict 是有序字典，“有序”是指它可以记住向字典里添加元素的顺序。因此，神经网络的正向传播只需按照添加元素的顺序调用各层的 forward() 方法就可以完成处理，而反向传播只需要按照相反的顺序调用各层即可。\n",
    "\n",
    "所以这里要做的事情仅仅是以正确的顺序连接各层，再按顺序（或者逆序）调用各层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "像这样通过将神经网络的组成元素以层的方式实现，可以轻松地构建神经网络。这个用层进行模块化的实现具有很大优点。因为想另外构建一个神经网络（例如 10 层、20 层，甚至更深的神经网络）时，只需像组装乐高积木那样添加必要的层就可以了。之后，通过各个层内部实现的正向传播和反向传播，就可以正确计算进行识别处理或学习所需的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 使用误差反向传播法的学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到目前为止，我们介绍了两种求梯度的方法。一种是基于数值微分的方法，计算很耗费时间；另一种是解析性地求解数学式的方法，它通过使用误差反向传播法，即使存在大量的参数，也可以高效地计算梯度。因此，后文将不再使用耗费时间的数值微分，而是使用误差反向传播法求梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了计算方便，使用 change_one_hot_label() 函数将 MNIST 的标签数据改为 one-hot 类型。同时，SoftmaxWithLoss 类中的 backward() 方法也需要进行相应更改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_one_hot_label(X):\n",
    "    T = np.zeros((X.size, 10))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 监督数据是one-hot-vector的情况下，转换为正确解标签的索引\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 损失\n",
    "        self.y = None # softmax的输出\n",
    "        self.t = None # 标签数据（one-hot vector）\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 监督数据是one-hot-vector的情况\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看一下基于 MNIST 数据库，使用了误差反向传播法的神经网络的学习的实现。和之前的实现相比，不同之处仅在于通过误差反向传播法求梯度这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.1359, 0.1429\n",
      "train acc, test acc | 0.9045, 0.9092\n",
      "train acc, test acc | 0.9223, 0.9249\n",
      "train acc, test acc | 0.9347, 0.9334\n",
      "train acc, test acc | 0.9440, 0.9428\n",
      "train acc, test acc | 0.9497, 0.9464\n",
      "train acc, test acc | 0.9521, 0.9494\n",
      "train acc, test acc | 0.9585, 0.9555\n",
      "train acc, test acc | 0.9619, 0.9588\n",
      "train acc, test acc | 0.9644, 0.9607\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from collections import OrderedDict\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_train = x_train.astype(np.float32) / 255.0\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "x_test = x_test.astype(np.float32) / 255.0\n",
    "\n",
    "t_train = change_one_hot_label(t_train)\n",
    "t_test = change_one_hot_label(t_test)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 6000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 通过误差反向传播法求梯度\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 更新参数\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print('train acc, test acc | {0:.4f}, {1:.4f}'.format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后可以用 Matplotlib 将训练过程中的训练集和测试集精度绘制出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
