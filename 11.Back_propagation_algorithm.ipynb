{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过数值微分计算了神经网络的权重参数的梯度（严格来说，是损失函数关于权重参数的梯度）。数值微分虽然简单，也容易实现，但缺点是计算上比较费时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要正确理解误差反向传播法，有两种方法：\n",
    "\n",
    "- 基于数学式\n",
    "- 基于计算图（computational graph）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 用计算图求解\n",
    "\n",
    "**计算图** 是表达和评估数学表达式的一种方式，它被定义为有向图，其中节点对应于数学运算。\n",
    "\n",
    "对于方程 $ z = x + y $ 的计算图如下：\n",
    "\n",
    "![img](images/chapter11/computational_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题1： 在超市买了 2 个 100 元一个的苹果，额外的消费税是 10%，请计算支付金额。\n",
    "\n",
    "![img](images/chapter11/apple_forward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "综上，用计算图解题的情况下，需要按如下流程进行。\n",
    "\n",
    "\n",
    "1. 构建计算图。\n",
    "\n",
    "\n",
    "2. 在计算图上，从左向右进行计算。\n",
    "\n",
    "这里的第2 歩“从左向右进行计算”是正向传播（forward propagation）。正向传播是从计算图出发点到结束点的传播。而从右向左的传播被称为反向传播（backward propagation），它将在接下来的导数计算中发挥重要作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为何用计算图解题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 无论全局是多么复杂的计算，都可以通过局部计算使各个节点致力于简单的计算，从而简化问题。\n",
    "\n",
    "\n",
    "- 利用计算图可以将中间的计算结果全部保存起来。\n",
    "\n",
    "\n",
    "- 可以通过反向传播高效计算导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，假设我们想知道苹果价格的上涨会在多大程度上影响最终的支付金额，即求“支付金额关于苹果的价格的导数”。设苹果的价格为 $x$，支付金额为 $L$，则相当于求 $\\frac{\\partial L}{\\partial x}$。导数的数值可以通过计算图的反向传播求出来。\n",
    "\n",
    "\n",
    "![img](images/chapter11/apple_backward.png)\n",
    "\n",
    "从这个结果中可知，“支付金额关于苹果的价格的导数”的值是2.2。这意味着，如果苹果的价格上涨 1 元，最终的支付金额会增加 2.2 元。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里只求了关于苹果的价格的导数，不过“支付金额关于消费税的导数”、“支付金额关于苹果的个数的导数”等也都可以用同样的方式算出来。并且，计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数。\n",
    "\n",
    "综上，计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 链式法则\n",
    "\n",
    "复合函数是由多个函数构成的函数。比如，$z = (x + y)^2$ 是由式（5.1）所示的两个式子构成的。\n",
    "\n",
    "$$ z= t^2 $$\n",
    "$$ t = x + y $$\n",
    "\n",
    "如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示。\n",
    "\n",
    "$z$ 关于 $x$ 的导数$ \\frac{\\partial z}{\\partial x}$，可以用 $z$ 关于 $t$ 的导数 $\\frac{\\partial z}{\\partial t}$ 和 $t$ 关于 $x$ 的导数 $\\frac{\\partial t}{\\partial x}$ 的乘积表示。\n",
    "$$ \\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t}\\frac{\\partial t}{\\partial x} = 2t\\cdot 1 = 2(x+y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算图的反向传播从右到左传播信号。反向传播的计算顺序是，先将节点的输入信号乘以节点的局部导数（偏导数），然后再传递给下一个节点。\n",
    "\n",
    "![img](images/chapter11/chain1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将求导结果带入上图中得到，\n",
    "![img](images/chapter11/chain2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 加法层的反向传播\n",
    "\n",
    "以 $z = x + y$ 为对象，观察它的反向传播。$z = x + y$ 的导数可解析性地计算出来。\n",
    "\n",
    "$$ \\frac{\\partial z}{\\partial x} = 1$$\n",
    "\n",
    "$$ \\frac{\\partial z}{\\partial y} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播将从上游传过来的导数（本例中是$ \\frac{\\partial L}{\\partial z}$）乘以1，然后传向下游。也就是说，因为加法节点的反向传播只乘以1，所以输入的值会原封不动地流向下一个节点。\n",
    "\n",
    "![img](images/chapter11/add_node.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们实现加法节点的加法层，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # 正向传播\n",
    "    def forward(self, x, y):\n",
    "        return x + y\n",
    "\n",
    "    # 反向传播\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        # return dout, dout\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加法层不需要特意进行初始化，所以 \\_\\_init\\_\\_() 中什么也不运行（pass语句表示“什么也不运行”）。加法层的 forward() 接收 x 和 y 两个参数，将它们相加后输出。backward() 将上游传来的导数（dout）原封不动地传递给下游。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 乘法层的反向传播\n",
    "\n",
    "这里我们考虑 $z = xy$。这个式的导数用下式表示：\n",
    "\n",
    "$$ \\frac{\\partial z}{\\partial x} = y$$\n",
    "\n",
    "$$ \\frac{\\partial z}{\\partial y} = x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "乘法的反向传播会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游。翻转值表示一种翻转关系，如下图所示，正向传播时信号是 x 的话，反向传播时则是 y；正向传播时信号是 y 的话，反向传播时则是 x。\n",
    "\n",
    "![img](images/chapter11/multi_node.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    \n",
    "    # 正向传播\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return x * y\n",
    "\n",
    "    # 反向传播\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y # 翻转x和y\n",
    "        dy = dout * self.x\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\_\\_init\\_\\_()中会初始化实例变量 x 和 y ，它们用于保存正向传播时的输入值。forward() 接收 x 和 y 两个参数，将它们相乘后输出。backward()将从上游传来的导数（dout）乘以正向传播的翻转值，然后传给下游。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们使用 MulLayer 类实现前面的购买苹果的例子：\n",
    "\n",
    "![img](images/chapter11/apple_backward_result.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(price) # 220"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于各个变量的导数可由backward()求出。这里调用backward()的顺序与调用forward()的顺序相反。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 200\n",
      "2.2 110.00000000000001\n"
     ]
    }
   ],
   "source": [
    "# backward\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "print(dapple_price, dtax)\n",
    "\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "print(dapple, dapple_num) # 2.2 110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 激活函数层的反向传播\n",
    "\n",
    "现在，我们将计算图的思路应用到神经网络中。这里，我们把构成神经网络的层实现为一个类。先来实现激活函数的 ReLU 层和 Sigmoid 层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 ReLU层\n",
    "\n",
    "激活函数ReLU（Rectified Linear Unit）由下式表示，\n",
    "\n",
    "$$ y = \\begin{cases}\n",
    "x&x > 0 \\\\\n",
    "0&x <= 0\n",
    "\\end{cases} $$\n",
    "\n",
    "可以求出 $y$ 关于 $x$ 的导数，\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial x} = \\begin{cases}\n",
    "1&x > 0 \\\\\n",
    "0&x <= 0\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果正向传播时的输入 $x$ 大于0，则反向传播会将上游的值原封不动地传给下游。反过来，如果正向传播时的 $x$ 小于等于0，则反向传播中传给下游的信号将停在此处。计算图如图所示，\n",
    "\n",
    "![img](images/chapter11/relu.png)\n",
    "\n",
    "现在我们来实现ReLU层。在神经网络的层的实现中，一般假定 forward() 和 backward() 的参数是NumPy数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu 类有实例变量mask，这个变量是由 True/False 构成的 NumPy 数组，它会把正向传播时的输入 $x$ 的元素中小于等于0 的地方保存为 True，其它地方（大于0 的元素）保存为 False。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 3.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array( [[1.0, -0.5], [-2.0, 3.0]] )\n",
    "\n",
    "Relu().forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Sigmoid层\n",
    "\n",
    "sigmoid 函数公式为\n",
    "$$ y = \\frac{1}{1 + \\exp (-x)} $$\n",
    "\n",
    "用计算图表示的话如图所示，\n",
    "\n",
    "![img](images/chapter11/sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们进行计算图的反向传播。\n",
    "\n",
    "- 步骤1\n",
    "      “/”节点表示 y = 1/x，它的导数可以解析性地表示为下式。\n",
    "$$\\frac{\\partial y}{\\partial x} = - \\frac{1}{x^2} = -y^2$$\n",
    "- 步骤2\n",
    "      “+”节点将上游的值原封不动地传给下游。\n",
    "- 步骤3\n",
    "      “exp”节点表示 y = exp(x)，它的导数为\n",
    "$$ \\frac{\\partial y}{\\partial x} = \\exp(x) $$\n",
    "- 步骤4\n",
    "      “×”节点将正向传播时的值翻转后做乘法运算。因此，这里要乘以−1。\n",
    "\n",
    "因此，Sigmoid层的反向传播计算图为：\n",
    "\n",
    "\n",
    "![img](images/chapter11/sigmoid_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid 层的反向传播输出为 $\\frac{\\partial L}{\\partial y}y^2\\exp(-x)$，可以进一步整理如下：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial y}y^2\\exp(-x) = \\frac{\\partial L}{\\partial y}y\\frac{1}{1 + \\exp (-x)}\\exp(-x) = \\frac{\\partial L}{\\partial y}y(1-y)$$\n",
    "\n",
    "![img](images/chapter11/sigmoid_result_simple.png)\n",
    "\n",
    "用 Python 代码实现sigmoid层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None    # 保存正向传播结果\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.out = 1 / (1 + np.exp(-x))\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.out * (1.0 - self.out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 全连接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑一个简单的神经网络全连接层计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(2) # 输入\n",
    "W = np.random.rand(2,3) # 权重\n",
    "B = np.random.rand(3) # 偏置\n",
    "\n",
    "Y = np.dot(X, W) + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个计算过程可以用计算图表示出来。之前我们见到的计算图中各个节点间流动的是标量，而这个例子中各个节点间传播的是矩阵。\n",
    "\n",
    "![img](images/chapter11/affine_forward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以矩阵为对象的反向传播，按矩阵的各个元素进行计算时，步骤和以标量为对象的计算图相同。\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y}\\cdot W^T $$\n",
    "$$ \\frac{\\partial L}{\\partial W} = X^T\\cdot \\frac{\\partial L}{\\partial Y} $$\n",
    "\n",
    "神经网络计算图的反向传播：\n",
    "\n",
    "![img](images/chapter11/affine_backward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们考虑 N 个数据一起进行正向传播的情况，也就是批版本的全连接层。先给出批版本的全连接层的计算图，\n",
    "\n",
    "![img](images/chapter11/affine_batch_backward.png)\n",
    "\n",
    "与之前不同的是，现在输入 $X$ 的形状是(N, 2)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Softmax-with-loss层\n",
    "\n",
    "下面来实现 Softmax 层。考虑到这里也包含作为损失函数的交叉熵误差（cross entropy error），所以称为“Softmax-with-Loss 层”。Softmax-with-Loss 层（Softmax函数和交叉熵误差）的计算图如图所示。\n",
    "\n",
    "![img](images/chapter11/softmax_with_loss.png)\n",
    "\n",
    "Softmax 层将输入值归一化（将输出值的和调整为1）之后再输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 正向传播\n",
    "\n",
    "softmax 函数可由下式表示\n",
    "\n",
    "$$ y_k = \\frac{\\exp (a_k)}{\\sum_{i=1}^n \\exp (a_i)} $$\n",
    "\n",
    "在Softmax层的计算图中，指数的和记做 $S$，最终的输出记为 $(y_1,y_2,y_3)$\n",
    "\n",
    "交叉熵误差(cross entropy error)可由下式表示，\n",
    "\n",
    "$$ L = -\\sum_k t_k \\log y_k$$\n",
    "\n",
    "交叉熵误差层的最终输出记为 $ -(t_1 \\log y_1 + t_2 \\log y_2 + t_3 \\log y_3) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 反向传播\n",
    "\n",
    "首先是 Cross Entropy Error 层的反向传播，\n",
    "![img](images/chapter11/cross_entropy_backward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中注意“ log”节点的反向传播遵从：\n",
    "$$ y= log(x)$$\n",
    "$$ \\frac{\\partial y}{\\partial x} = \\frac{1}{x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是Softmax 层的反向传播的步骤，稍有些复杂。我们将它分为两部分来讨论，\n",
    "\n",
    "![img](images/chapter11/softmax_backward1.png)\n",
    "\n",
    "1. “×”节点将正向传播的值翻转后相乘。\n",
    "\n",
    "$$ -\\frac{t_1}{y1}\\exp(a_1) = -t_1\\frac{S}{\\exp (a_1)}\\exp(a_1)=-t_1 S $$\n",
    "\n",
    "2. 正向传播时若有分支流出，则反向传播时它们的反向传播的值会相加。\n",
    "\n",
    "$$ (-t_1S - t_2S - t_3S)\\cdot -(\\frac{1}{S})^2 = \\frac{1}{S}(t_1+t_2+t_3) = \\frac{1}{S} $$\n",
    "\n",
    "这里，(t1, t2, t3) 是 one-hot 向量，意味着(t1, t2, t3)中只有一个元素是1，其余都是0。因此，$t_1+t_2+t_3 = 1$\n",
    "\n",
    "3. “+”节点原封不动地传递上游的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](images/chapter11/softmax_backward2.png)\n",
    "\n",
    "1. “×”节点将值翻转后相乘。\n",
    "\n",
    "$$ -\\frac{t_1}{y_1} \\frac{1}{S} = -\\frac{t_1}{\\exp a_1} $$\n",
    "\n",
    "2. 根据“exp”节点的关系式 $ \\frac{\\partial y}{\\partial x} = \\exp(x) $，向两个分支输入的和乘以 $\\exp(a_1)$ 后的值就是我们要求的反向传播。\n",
    "\n",
    "$$ (\\frac{1}{S} - \\frac{t_1}{\\exp(a_1)}) \\exp(a_1) = \\frac{\\exp(a_1)}{S} - t_1 = y_1 - t_1 $$\n",
    "\n",
    "3. 剩下的 $a_2$、$a_3$ 也可以按照相同的步骤求出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax 层的反向传播得到了 $(y_1 − t_1, y_2 − t_2, y_3 − t_3)$ 这样“漂亮”的结果。由于 $(y1, y2, y3)$ 是 Softmax 层的输出，$(t1, t2, t3)$ 是标签数据，所以 $(y_1 − t_1, y_2 − t_2, y_3 − t_3)$ 是 Softmax 层的输出和标签的差分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如，Softmax 层的输出是 $y=(0.3, 0.2, 0.5)$ ，标签数据是 $t =(0, 1, 0)$。因为正确解标签处的概率是 0.2（20%），这时的神经网络未能进行正确的识别。此时，Softmax 层的反向传播传递的是 (0.3, −0.8, 0.5) 这样一个大的误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在来进行Softmax-with-Loss 层的实现，实现代码如下所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 损失\n",
    "        self.y = None # softmax的输出\n",
    "        self.t = None # 标签数据（one-hot vector）\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意反向传播时，将要传播的值除以批的大小（batch_size）后，传递给前面的层的是单个数据的误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 误差反向传播的实现\n",
    "\n",
    "回顾神经网络学习的步骤如下所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 前提\n",
    "      神经网络中有合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为学习。神经网络的学习分为下面4 个步骤。\n",
    "      \n",
    "- 步骤1（mini-batch）\n",
    "      从训练数据中随机选择一部分数据。\n",
    "\n",
    "- 步骤2（计算梯度）\n",
    "      计算损失函数关于各个权重参数的梯度。\n",
    "\n",
    "- 步骤3（更新参数）\n",
    "      将权重参数沿梯度方向进行微小的更新。\n",
    "\n",
    "- 步骤4（重复）\n",
    "      重复步骤1、步骤2、步骤3。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一章中，我们利用数值微分求得了权重参数的梯度。数值微分虽然实现简单，但是计算要耗费较多的时间。和需要花费较多时间的数值微分不同，误差反向传播法可以快速高效地计算梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 对应误差反向传播法的神经网络的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过像组装乐高积木一样组装上一节中实现的层，可以构建神经网络。本节我们将通过组装已经实现的层来构建神经网络，这里我们将 2 层神经网络实现为TwoLayerNet 类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 类源代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OrderedDict 是有序字典，“有序”是指它可以记住向字典里添加元素的顺序。因此，神经网络的正向传播只需按照添加元素的顺序调用各层的 forward() 方法就可以完成处理，而反向传播只需要按照相反的顺序调用各层即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到目前为止，我们介绍了两种求梯度的方法。一种是基于数值微分的方法，计算很耗费时间；另一种是解析性地求解数学式的方法，它通过使用误差反向传播法，即使存在大量的参数，也可以高效地计算梯度。因此，后文将不再使用耗费时间的数值微分，而是使用误差反向传播法求梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看一下基于 MNIST 数据库，使用了误差反向传播法的神经网络的学习的实现。和之前的实现相比，不同之处仅在于通过误差反向传播法求梯度这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TwoLayerNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4f228e78ddcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mnetwork\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTwoLayerNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0miters_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TwoLayerNet' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 通过误差反向传播法求梯度\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
